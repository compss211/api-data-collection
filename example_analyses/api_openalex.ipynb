{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6caa532f-d502-4576-9ee6-5ad88ca0c550",
   "metadata": {},
   "source": [
    "# Python Web APIs: Accessing Academic Research with OpenAlex\n",
    "\n",
    "* * * \n",
    "\n",
    "### Icons used in this notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive excercise. We'll work through these in the workshop!<br>\n",
    "‚ö†Ô∏è **Warning**: Heads-up about tricky stuff or common mistakes.<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "üé¨ **Demo**: Showing off something more advanced ‚Äì so you know what Python can be used for!<br>\n",
    "\n",
    "### Learning Objectives\n",
    "1. [Introduction to OpenAlex API](#openalex)\n",
    "2. [Searching for Research Works](#works)\n",
    "3. [Author and Institution Analysis](#authors)\n",
    "4. [Research Topics and Concepts](#topics)\n",
    "5. [Data Analysis with Academic Metrics](#analysis)\n",
    "6. [Demo: Research Network Visualization](#demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0379359-694d-491c-ba18-fa80ea3c65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2eeb34-dab0-464c-8fca-568624fe2cbb",
   "metadata": {},
   "source": [
    "<a id='openalex'></a>\n",
    "\n",
    "# OpenAlex API\n",
    "\n",
    "OpenAlex is a free and open catalog of scholarly papers, authors, institutions, and more. It's the successor to Microsoft Academic Graph and provides access to:\n",
    "\n",
    "- **Works**: Scholarly papers, books, datasets, etc.\n",
    "- **Authors**: Researcher profiles and affiliations\n",
    "- **Sources**: Journals, conferences, and repositories\n",
    "- **Institutions**: Universities, companies, and research organizations\n",
    "- **Topics**: Research areas and subject classifications\n",
    "- **Publishers**: Academic publishers\n",
    "- **Funders**: Funding organizations\n",
    "\n",
    "üí° **Tip**: OpenAlex is completely free and requires no API key! However, adding your email to requests is recommended for best performance and helps them track usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639eb32-15e0-48fc-9cb5-3bd804fae0f9",
   "metadata": {},
   "source": [
    "## Setting Up API Access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da084101-19bc-46c1-9722-317d0514da5e",
   "metadata": {},
   "source": [
    "While OpenAlex doesn't require an API key, it's recommended to include your email in requests for better performance and to help them understand usage patterns. The API has a daily limit of 100,000 requests per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a9823a-4e85-41ff-b321-4f9821359821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "def get_email_for_openalex():\n",
    "    config_file_path = os.path.expanduser(\"~/.notebook-api-keys\")\n",
    "    config = configparser.ConfigParser(interpolation=None)\n",
    "    \n",
    "    # Try reading the existing config file\n",
    "    if os.path.exists(config_file_path):\n",
    "        config.read(config_file_path)\n",
    "    \n",
    "    # Check if email is already stored\n",
    "    if config.has_option(\"API_KEYS\", \"OPENALEX_EMAIL\"):\n",
    "        update_email = input(\"Email for OpenAlex already exists. Do you want to update it? (y/n): \").lower()\n",
    "        if update_email == 'n':\n",
    "            return config.get(\"API_KEYS\", \"OPENALEX_EMAIL\")\n",
    "    \n",
    "    # Get email from user\n",
    "    email = input(\"Enter your email for OpenAlex API (optional but recommended): \")\n",
    "    \n",
    "    if email:\n",
    "        # Save the email in the config file\n",
    "        if not config.has_section(\"API_KEYS\"):\n",
    "            config.add_section(\"API_KEYS\")\n",
    "        config.set(\"API_KEYS\", \"OPENALEX_EMAIL\", email)\n",
    "        \n",
    "        with open(config_file_path, \"w\") as f:\n",
    "            config.write(f)\n",
    "    \n",
    "    return email\n",
    "\n",
    "# Get email for better API performance\n",
    "user_email = get_email_for_openalex()\n",
    "\n",
    "print(\"OpenAlex setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b395a-443f-4e40-91f7-8fcb5c33bbbe",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAlex API Client\n",
    "\n",
    "Unlike other APIs we've used, OpenAlex doesn't have an official Python client, but its REST API is straightforward to use directly. We'll create a simple wrapper class to make our requests easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce57534-fe5d-45a4-bb97-f2df8dc3d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAlexAPI:\n",
    "    def __init__(self, email=None):\n",
    "        self.base_url = \"https://api.openalex.org\"\n",
    "        self.email = email\n",
    "        # Rate limiting - be respectful!\n",
    "        self.last_request = 0\n",
    "        self.min_interval = 0.1  # 100ms between requests\n",
    "    \n",
    "    def _make_request(self, endpoint, params=None):\n",
    "        \"\"\"Make a rate-limited request to the API\"\"\"\n",
    "        # Rate limiting\n",
    "        elapsed = time.time() - self.last_request\n",
    "        if elapsed < self.min_interval:\n",
    "            time.sleep(self.min_interval - elapsed)\n",
    "        \n",
    "        if params is None:\n",
    "            params = {}\n",
    "        \n",
    "        # Add email if provided\n",
    "        if self.email:\n",
    "            params['mailto'] = self.email\n",
    "        \n",
    "        url = f\"{self.base_url}/{endpoint}\"\n",
    "        response = requests.get(url, params=params)\n",
    "        self.last_request = time.time()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Error {response.status_code}: {response.text}\")\n",
    "            return None\n",
    "    \n",
    "    def search_works(self, query=None, filters=None, per_page=25, page=1):\n",
    "        \"\"\"Search for academic works\"\"\"\n",
    "        params = {\n",
    "            'per-page': per_page,\n",
    "            'page': page\n",
    "        }\n",
    "        \n",
    "        if query:\n",
    "            params['search'] = query\n",
    "        \n",
    "        if filters:\n",
    "            params['filter'] = filters\n",
    "        \n",
    "        return self._make_request('works', params)\n",
    "    \n",
    "    def get_work(self, work_id):\n",
    "        \"\"\"Get detailed information about a specific work\"\"\"\n",
    "        return self._make_request(f'works/{work_id}')\n",
    "    \n",
    "    def search_authors(self, query=None, filters=None, per_page=25):\n",
    "        \"\"\"Search for authors\"\"\"\n",
    "        params = {'per-page': per_page}\n",
    "        \n",
    "        if query:\n",
    "            params['search'] = query\n",
    "        \n",
    "        if filters:\n",
    "            params['filter'] = filters\n",
    "        \n",
    "        return self._make_request('authors', params)\n",
    "    \n",
    "    def get_author(self, author_id):\n",
    "        \"\"\"Get detailed information about a specific author\"\"\"\n",
    "        return self._make_request(f'authors/{author_id}')\n",
    "    \n",
    "    def search_institutions(self, query=None, filters=None, per_page=25):\n",
    "        \"\"\"Search for institutions\"\"\"\n",
    "        params = {'per-page': per_page}\n",
    "        \n",
    "        if query:\n",
    "            params['search'] = query\n",
    "        \n",
    "        if filters:\n",
    "            params['filter'] = filters\n",
    "        \n",
    "        return self._make_request('institutions', params)\n",
    "    \n",
    "    def search_topics(self, query=None, per_page=25):\n",
    "        \"\"\"Search for research topics\"\"\"\n",
    "        params = {'per-page': per_page}\n",
    "        \n",
    "        if query:\n",
    "            params['search'] = query\n",
    "        \n",
    "        return self._make_request('topics', params)\n",
    "\n",
    "# Initialize the API client\n",
    "openalex = OpenAlexAPI(user_email)\n",
    "print(\"OpenAlex API client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13137c6-c776-4da0-b1a2-195726244be5",
   "metadata": {},
   "source": [
    "Perfect! We are now ready to explore academic research data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a18eac-436d-43ef-8151-6bfc85a74a8d",
   "metadata": {},
   "source": [
    "<a id='works'></a>\n",
    "\n",
    "# Searching for Research Works\n",
    "\n",
    "Let's start by searching for academic papers on a specific topic. OpenAlex calls academic papers \"works\" and includes journal articles, conference papers, books, datasets, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d14b2a-67f3-42b6-b383-1eb78962a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for works about artificial intelligence\n",
    "ai_works = openalex.search_works(query=\"artificial intelligence\", per_page=50)\n",
    "\n",
    "if ai_works and 'results' in ai_works:\n",
    "    print(f\"Found {ai_works['meta']['count']} total works about artificial intelligence\")\n",
    "    print(f\"Retrieved {len(ai_works['results'])} works\")\n",
    "    \n",
    "    # Look at the first work\n",
    "    first_work = ai_works['results'][0]\n",
    "    print(f\"\\nFirst work:\")\n",
    "    print(f\"Title: {first_work['title']}\")\n",
    "    print(f\"Publication Year: {first_work['publication_year']}\")\n",
    "    print(f\"Citations: {first_work['cited_by_count']}\")\n",
    "    print(f\"Type: {first_work['type']}\")\n",
    "    print(f\"Open Access: {first_work['open_access']['is_oa']}\")\n",
    "else:\n",
    "    print(\"No works found or API error occurred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eded794-5f47-4848-9c59-9ba325efecaf",
   "metadata": {},
   "source": [
    "Let's examine the structure of a single work to understand what data is available to us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582b805-46fb-48f4-8b0c-fc5585b7b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the structure of the first work (excluding some verbose fields)\n",
    "if ai_works and 'results' in ai_works:\n",
    "    work = ai_works['results'][0]\n",
    "    \n",
    "    # Display key information\n",
    "    print(\"Available fields:\")\n",
    "    for key in work.keys():\n",
    "        if key not in ['abstract_inverted_index', 'referenced_works', 'related_works']:\n",
    "            print(f\"  {key}: {type(work[key])}\")\n",
    "    \n",
    "    print(f\"\\nAuthors ({len(work['authorships'])}):\")    \n",
    "    for i, authorship in enumerate(work['authorships'][:3]):  # First 3 authors\n",
    "        author = authorship['author']\n",
    "        print(f\"  {i+1}. {author['display_name']} (ID: {author['id']})\")\n",
    "        if authorship['institutions']:\n",
    "            inst = authorship['institutions'][0]\n",
    "            print(f\"     Institution: {inst['display_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73610bbb-e547-4dac-9d55-be815fba4009",
   "metadata": {},
   "source": [
    "## Converting API Results to a DataFrame\n",
    "\n",
    "Let's convert our results into a pandas DataFrame for easier analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176bb5a8-53db-4244-a525-bd836b902ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def works_to_dataframe(works_response):\n",
    "    \"\"\"Convert OpenAlex works response to a clean DataFrame\"\"\"\n",
    "    if not works_response or 'results' not in works_response:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    data = []\n",
    "    for work in works_response['results']:\n",
    "        # Extract basic information\n",
    "        row = {\n",
    "            'id': work['id'],\n",
    "            'title': work['title'],\n",
    "            'publication_year': work['publication_year'],\n",
    "            'cited_by_count': work['cited_by_count'],\n",
    "            'type': work['type'],\n",
    "            'is_open_access': work['open_access']['is_oa'],\n",
    "            'oa_type': work['open_access'].get('oa_type', 'closed'),\n",
    "            'num_authors': len(work['authorships']),\n",
    "            'num_concepts': len(work['concepts']),\n",
    "            'primary_location': None,\n",
    "            'journal': None,\n",
    "            'publisher': None\n",
    "        }\n",
    "        \n",
    "        # Extract publication venue information\n",
    "        if work['primary_location']:\n",
    "            primary = work['primary_location']\n",
    "            row['primary_location'] = primary.get('display_name')\n",
    "            if primary.get('source'):\n",
    "                row['journal'] = primary['source'].get('display_name')\n",
    "        \n",
    "        # Extract first author\n",
    "        if work['authorships']:\n",
    "            first_author = work['authorships'][0]['author']\n",
    "            row['first_author'] = first_author['display_name']\n",
    "            row['first_author_id'] = first_author['id']\n",
    "        else:\n",
    "            row['first_author'] = None\n",
    "            row['first_author_id'] = None\n",
    "        \n",
    "        # Extract top concept\n",
    "        if work['concepts']:\n",
    "            top_concept = max(work['concepts'], key=lambda x: x['score'])\n",
    "            row['top_concept'] = top_concept['display_name']\n",
    "            row['top_concept_score'] = top_concept['score']\n",
    "        else:\n",
    "            row['top_concept'] = None\n",
    "            row['top_concept_score'] = None\n",
    "        \n",
    "        data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Convert our AI works to a DataFrame\n",
    "if ai_works:\n",
    "    df_ai = works_to_dataframe(ai_works)\n",
    "    print(f\"Created DataFrame with {len(df_ai)} works\")\n",
    "    df_ai.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e6202-f3f8-4d43-9601-c0fb0404ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the DataFrame metadata\n",
    "if not df_ai.empty:\n",
    "    df_ai.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc28bc7d-d3dd-48ff-99d8-20b9db7046b0",
   "metadata": {},
   "source": [
    "## ü•ä Challenge: Explore a Research Topic\n",
    "\n",
    "- Choose a research topic that interests you\n",
    "- Search for works on that topic and convert to a DataFrame\n",
    "- What are the most cited papers? What's the publication year range?\n",
    "- How many are open access vs. closed access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c920bc-bf8c-48a4-a34e-d10e4a23476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741048f9-1f01-434a-b5e8-fc51583c8d8e",
   "metadata": {},
   "source": [
    "<a id='authors'></a>\n",
    "\n",
    "# Author and Institution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8b137d-4365-4994-beee-04b8e06cc9cf",
   "metadata": {},
   "source": [
    "Let's explore author information and institutional affiliations. We can search for specific authors or analyze the most prolific researchers in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959dc92b-c33f-41b5-b271-15eb17a5a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed information about the most cited author from our AI dataset\n",
    "if not df_ai.empty:\n",
    "    # Find the work with the most citations\n",
    "    most_cited_work = df_ai.loc[df_ai['cited_by_count'].idxmax()]\n",
    "    author_id = most_cited_work['first_author_id']\n",
    "    \n",
    "    if author_id:\n",
    "        # Clean the author ID (remove URL prefix)\n",
    "        clean_author_id = author_id.split('/')[-1] if '/' in author_id else author_id\n",
    "        \n",
    "        author_info = openalex.get_author(clean_author_id)\n",
    "        \n",
    "        if author_info:\n",
    "            print(f\"Author: {author_info['display_name']}\")\n",
    "            print(f\"Works count: {author_info['works_count']}\")\n",
    "            print(f\"Cited by count: {author_info['cited_by_count']}\")\n",
    "            print(f\"h-index: {author_info['summary_stats']['h_index']}\")\n",
    "            print(f\"i10-index: {author_info['summary_stats']['i10_index']}\")\n",
    "            \n",
    "            # Show current affiliations\n",
    "            if author_info['affiliations']:\n",
    "                print(f\"\\nCurrent affiliations:\")\n",
    "                for affiliation in author_info['affiliations'][:3]:\n",
    "                    institution = affiliation['institution']\n",
    "                    print(f\"  - {institution['display_name']} ({institution.get('country_code', 'Unknown')})\")\n",
    "            \n",
    "            # Show research areas (concepts)\n",
    "            if author_info['x_concepts']:\n",
    "                print(f\"\\nTop research areas:\")\n",
    "                for concept in author_info['x_concepts'][:5]:\n",
    "                    print(f\"  - {concept['display_name']} (score: {concept['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3681696d-752e-4d11-8f67-af5ae03bb73b",
   "metadata": {},
   "source": [
    "## Institution Analysis\n",
    "\n",
    "Let's search for top institutions in artificial intelligence research:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358187a6-420d-4317-9429-5a021edb1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for institutions with AI research, sorted by citation count\n",
    "top_ai_institutions = openalex.search_institutions(\n",
    "    filters=\"topics.id:T11597\",  # AI topic ID in OpenAlex\n",
    "    per_page=20\n",
    ")\n",
    "\n",
    "if top_ai_institutions and 'results' in top_ai_institutions:\n",
    "    print(\"Top institutions in AI research (by citation count):\")\n",
    "    print(f\"Total found: {top_ai_institutions['meta']['count']}\\n\")\n",
    "    \n",
    "    institutions_data = []\n",
    "    for i, institution in enumerate(top_ai_institutions['results'][:10]):\n",
    "        institutions_data.append({\n",
    "            'rank': i + 1,\n",
    "            'name': institution['display_name'],\n",
    "            'country': institution.get('country_code', 'Unknown'),\n",
    "            'type': institution.get('type', 'Unknown'),\n",
    "            'works_count': institution['works_count'],\n",
    "            'cited_by_count': institution['cited_by_count']\n",
    "        })\n",
    "        \n",
    "        print(f\"{i+1:2d}. {institution['display_name']} ({institution.get('country_code', 'Unknown')})\")\n",
    "        print(f\"    Works: {institution['works_count']:,}, Citations: {institution['cited_by_count']:,}\")\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    df_institutions = pd.DataFrame(institutions_data)\n",
    "else:\n",
    "    print(\"No institutions found or API error occurred\")\n",
    "    df_institutions = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60086e2-a79b-47f3-a480-e58339194f3f",
   "metadata": {},
   "source": [
    "<a id='topics'></a>\n",
    "\n",
    "# Research Topics and Concepts\n",
    "\n",
    "OpenAlex automatically classifies research works by topic. Let's explore the topic landscape in our field of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ff11f-7049-488f-9d96-f689c430a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for AI-related topics\n",
    "ai_topics = openalex.search_topics(query=\"artificial intelligence\", per_page=20)\n",
    "\n",
    "if ai_topics and 'results' in ai_topics:\n",
    "    print(\"AI-related research topics:\")\n",
    "    print(f\"Found {ai_topics['meta']['count']} total topics\\n\")\n",
    "    \n",
    "    for i, topic in enumerate(ai_topics['results'][:10]):\n",
    "        print(f\"{i+1:2d}. {topic['display_name']}\")\n",
    "        print(f\"    Description: {topic.get('description', 'No description')[:100]}...\")\n",
    "        print(f\"    Works: {topic['works_count']:,}, Citations: {topic['cited_by_count']:,}\")\n",
    "        print(f\"    Field: {topic.get('field', {}).get('display_name', 'Unknown')}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No topics found or API error occurred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c2f903-afb6-4f74-a542-711c5e3d149e",
   "metadata": {},
   "source": [
    "## Analyzing Concepts in Our Dataset\n",
    "\n",
    "Let's analyze what concepts appear most frequently in our AI papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25dd533-b471-44fa-8739-0e7e0c756255",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_ai.empty:\n",
    "    # Analyze top concepts\n",
    "    concept_counts = df_ai['top_concept'].value_counts()\n",
    "    print(\"Most common research concepts:\")\n",
    "    print(concept_counts.head(10))\n",
    "    \n",
    "    # Visualize concept distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    concept_counts.head(10).plot(kind='barh', alpha=0.7)\n",
    "    plt.title('Top 10 Research Concepts')\n",
    "    plt.xlabel('Number of Papers')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    df_ai['top_concept_score'].hist(bins=20, alpha=0.7, color='green')\n",
    "    plt.title('Distribution of Concept Scores')\n",
    "    plt.xlabel('Concept Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7fb990-96de-49dd-9611-4b798884321a",
   "metadata": {},
   "source": [
    "<a id='analysis'></a>\n",
    "\n",
    "# Data Analysis with Academic Metrics\n",
    "\n",
    "Let's perform some comprehensive analysis on our research data, looking at trends over time, open access patterns, and citation distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b86f60e-480f-4635-ba84-3dff7addfe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more data for analysis - let's collect 200 AI papers\n",
    "all_ai_works = []\n",
    "\n",
    "# Get multiple pages of results\n",
    "for page in range(1, 5):  # Get 4 pages = 100 works\n",
    "    print(f\"Fetching page {page}...\")\n",
    "    works = openalex.search_works(\n",
    "        query=\"artificial intelligence\", \n",
    "        per_page=25, \n",
    "        page=page\n",
    "    )\n",
    "    \n",
    "    if works and 'results' in works:\n",
    "        all_ai_works.extend(works['results'])\n",
    "    \n",
    "    time.sleep(0.1)  # Be nice to the API\n",
    "\n",
    "print(f\"Collected {len(all_ai_works)} total works\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_ai_large = works_to_dataframe({'results': all_ai_works})\n",
    "print(f\"DataFrame shape: {df_ai_large.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39a319-9185-4740-99c0-f34c437b6e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive analysis\n",
    "if not df_ai_large.empty:\n",
    "    # Basic statistics\n",
    "    print(\"Dataset Overview:\")\n",
    "    print(f\"Total papers: {len(df_ai_large)}\")\n",
    "    print(f\"Year range: {df_ai_large['publication_year'].min()} - {df_ai_large['publication_year'].max()}\")\n",
    "    print(f\"Median citations: {df_ai_large['cited_by_count'].median():.0f}\")\n",
    "    print(f\"Open access papers: {df_ai_large['is_open_access'].sum()} ({df_ai_large['is_open_access'].mean()*100:.1f}%)\")\n",
    "    print(f\"Average authors per paper: {df_ai_large['num_authors'].mean():.1f}\")\n",
    "    \n",
    "    # Create comprehensive visualizations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Publications over time\n",
    "    yearly_counts = df_ai_large.groupby('publication_year').size()\n",
    "    yearly_counts.plot(ax=axes[0,0], kind='line', marker='o', linewidth=2)\n",
    "    axes[0,0].set_title('AI Publications Over Time')\n",
    "    axes[0,0].set_xlabel('Year')\n",
    "    axes[0,0].set_ylabel('Number of Papers')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Citation distribution\n",
    "    df_ai_large['cited_by_count'].hist(bins=50, ax=axes[0,1], alpha=0.7, color='skyblue')\n",
    "    axes[0,1].set_title('Citation Distribution')\n",
    "    axes[0,1].set_xlabel('Number of Citations')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    axes[0,1].set_yscale('log')\n",
    "    \n",
    "    # 3. Open Access over time\n",
    "    oa_by_year = df_ai_large.groupby('publication_year')['is_open_access'].agg(['sum', 'count'])\n",
    "    oa_rate = (oa_by_year['sum'] / oa_by_year['count'] * 100).dropna()\n",
    "    oa_rate.plot(ax=axes[0,2], kind='line', marker='s', color='green', linewidth=2)\n",
    "    axes[0,2].set_title('Open Access Rate Over Time')\n",
    "    axes[0,2].set_xlabel('Year')\n",
    "    axes[0,2].set_ylabel('Open Access Rate (%)')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Work types\n",
    "    type_counts = df_ai_large['type'].value_counts()\n",
    "    type_counts.plot(kind='pie', ax=axes[1,0], autopct='%1.1f%%')\n",
    "    axes[1,0].set_title('Distribution of Work Types')\n",
    "    axes[1,0].set_ylabel('')\n",
    "    \n",
    "    # 5. Author collaboration (number of authors)\n",
    "    df_ai_large['num_authors'].hist(bins=range(1, 21), ax=axes[1,1], alpha=0.7, color='orange')\n",
    "    axes[1,1].set_title('Author Collaboration Patterns')\n",
    "    axes[1,1].set_xlabel('Number of Authors')\n",
    "    axes[1,1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 6. Citations vs. Year scatter\n",
    "    scatter = axes[1,2].scatter(df_ai_large['publication_year'], df_ai_large['cited_by_count'], \n",
    "                               c=df_ai_large['is_open_access'], alpha=0.6, \n",
    "                               cmap='RdYlBu_r')\n",
    "    axes[1,2].set_title('Citations vs Publication Year')\n",
    "    axes[1,2].set_xlabel('Publication Year')\n",
    "    axes[1,2].set_ylabel('Citations')\n",
    "    axes[1,2].set_yscale('log')\n",
    "    plt.colorbar(scatter, ax=axes[1,2], label='Open Access')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ad4d2-d283-466b-a395-b56fd9817a3a",
   "metadata": {},
   "source": [
    "## ü•ä Challenge: Research Impact Analysis\n",
    "\n",
    "- Find the most impactful papers (highest cited_by_count)\n",
    "- Compare open access vs. closed access papers - which get cited more?\n",
    "- Analyze if there's a correlation between number of authors and citations\n",
    "- What publication venues (journals) appear most frequently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce99e3-4cf1-475e-a7a2-7f545f06d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4b3b3-2b4a-4a20-9a79-ca90d90262f8",
   "metadata": {},
   "source": [
    "<a id='demo'></a>\n",
    "\n",
    "# üé¨ Demo: Research Network Visualization\n",
    "\n",
    "Let's create a network visualization showing connections between authors, institutions, and research topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a7832-3186-45c6-8b43-e52c99486526",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4561fe76-6d07-46c5-bda7-9973e7631ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from collections import Counter\n",
    "\n",
    "def create_research_network(works_data, max_nodes=50):\n",
    "    \"\"\"Create a network of authors, institutions, and concepts\"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Collect data for network\n",
    "    author_papers = {}\n",
    "    concept_papers = {}\n",
    "    institution_authors = {}\n",
    "    \n",
    "    for work in works_data[:20]:  # Use first 20 works to keep network manageable\n",
    "        work_id = work['id']\n",
    "        \n",
    "        # Process authors and their institutions\n",
    "        for authorship in work['authorships'][:3]:  # Top 3 authors per paper\n",
    "            author = authorship['author']\n",
    "            author_name = author['display_name']\n",
    "            \n",
    "            # Add author node\n",
    "            if author_name not in author_papers:\n",
    "                author_papers[author_name] = []\n",
    "            author_papers[author_name].append(work_id)\n",
    "            \n",
    "            G.add_node(author_name, type='author', size=10)\n",
    "            \n",
    "            # Add institution connections\n",
    "            for institution in authorship['institutions'][:1]:  # Primary institution\n",
    "                inst_name = institution['display_name']\n",
    "                \n",
    "                # Simplify long institution names\n",
    "                if len(inst_name) > 30:\n",
    "                    inst_name = inst_name[:27] + '...'\n",
    "                \n",
    "                G.add_node(inst_name, type='institution', size=20)\n",
    "                G.add_edge(author_name, inst_name, type='affiliation')\n",
    "        \n",
    "        # Process concepts\n",
    "        for concept in work['concepts'][:2]:  # Top 2 concepts\n",
    "            if concept['score'] > 0.3:  # Only high-confidence concepts\n",
    "                concept_name = concept['display_name']\n",
    "                \n",
    "                if concept_name not in concept_papers:\n",
    "                    concept_papers[concept_name] = []\n",
    "                concept_papers[concept_name].append(work_id)\n",
    "                \n",
    "                G.add_node(concept_name, type='concept', size=15)\n",
    "                \n",
    "                # Connect authors to concepts\n",
    "                for authorship in work['authorships'][:2]:\n",
    "                    author_name = authorship['author']['display_name']\n",
    "                    G.add_edge(author_name, concept_name, type='research')\n",
    "    \n",
    "    # Add collaboration edges (authors who co-authored papers)\n",
    "    for work in works_data[:20]:\n",
    "        authors = [auth['author']['display_name'] for auth in work['authorships'][:3]]\n",
    "        for i in range(len(authors)):\n",
    "            for j in range(i+1, len(authors)):\n",
    "                if G.has_edge(authors[i], authors[j]):\n",
    "                    G[authors[i]][authors[j]]['weight'] = G[authors[i]][authors[j]].get('weight', 0) + 1\n",
    "                else:\n",
    "                    G.add_edge(authors[i], authors[j], type='collaboration', weight=1)\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Create the network\n",
    "if all_ai_works:\n",
    "    network = create_research_network(all_ai_works)\n",
    "    print(f\"Created network with {network.number_of_nodes()} nodes and {network.number_of_edges()} edges\")\n",
    "    \n",
    "    # Visualize the network\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Create layout\n",
    "    pos = nx.spring_layout(network, k=1, iterations=50)\n",
    "    \n",
    "    # Color nodes by type\n",
    "    node_colors = []\n",
    "    node_sizes = []\n",
    "    for node in network.nodes():\n",
    "        node_data = network.nodes[node]\n",
    "        if node_data['type'] == 'author':\n",
    "            node_colors.append('lightblue')\n",
    "            node_sizes.append(300)\n",
    "        elif node_data['type'] == 'institution':\n",
    "            node_colors.append('lightgreen')\n",
    "            node_sizes.append(500)\n",
    "        else:  # concept\n",
    "            node_colors.append('lightcoral')\n",
    "            node_sizes.append(400)\n",
    "    \n",
    "    # Draw the network\n",
    "    nx.draw_networkx_nodes(network, pos, node_color=node_colors, node_size=node_sizes, alpha=0.7)\n",
    "    \n",
    "    # Draw different types of edges with different colors\n",
    "    edge_colors = []\n",
    "    for edge in network.edges():\n",
    "        edge_data = network.edges[edge]\n",
    "        if edge_data['type'] == 'collaboration':\n",
    "            edge_colors.append('blue')\n",
    "        elif edge_data['type'] == 'affiliation':\n",
    "            edge_colors.append('green')\n",
    "        else:  # research\n",
    "            edge_colors.append('red')\n",
    "    \n",
    "    nx.draw_networkx_edges(network, pos, edge_color=edge_colors, alpha=0.5, width=0.8)\n",
    "    \n",
    "    # Add labels for important nodes only\n",
    "    important_nodes = {node: node for node in network.nodes() if network.degree(node) > 2}\n",
    "    nx.draw_networkx_labels(network, pos, important_nodes, font_size=8, font_weight='bold')\n",
    "    \n",
    "    plt.title('AI Research Network: Authors, Institutions, and Concepts', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Create legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='lightblue', markersize=10, label='Authors'),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', markersize=10, label='Institutions'),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='lightcoral', markersize=10, label='Concepts'),\n",
    "        Line2D([0], [0], color='blue', linewidth=2, label='Collaboration'),\n",
    "        Line2D([0], [0], color='green', linewidth=2, label='Affiliation'),\n",
    "        Line2D([0], [0], color='red', linewidth=2, label='Research Area')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Network statistics\n",
    "    print(f\"\\nNetwork Statistics:\")\n",
    "    print(f\"Number of authors: {len([n for n in network.nodes() if network.nodes[n]['type'] == 'author'])}\")\n",
    "    print(f\"Number of institutions: {len([n for n in network.nodes() if network.nodes[n]['type'] == 'institution'])}\")\n",
    "    print(f\"Number of concepts: {len([n for n in network.nodes() if network.nodes[n]['type'] == 'concept'])}\")\n",
    "    print(f\"Average clustering coefficient: {nx.average_clustering(network):.3f}\")\n",
    "    print(f\"Network density: {nx.density(network):.3f}\")\n",
    "    \n",
    "    # Most connected nodes\n",
    "    degree_centrality = nx.degree_centrality(network)\n",
    "    top_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    print(f\"\\nMost connected nodes:\")\n",
    "    for node, centrality in top_nodes:\n",
    "        node_type = network.nodes[node]['type']\n",
    "        print(f\"  {node} ({node_type}): {centrality:.3f}\")\n",
    "else:\n",
    "    print(\"No data available for network creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebed890-3232-4399-ac6d-ebefc948f6d8",
   "metadata": {},
   "source": [
    "## Advanced Research Data Collection\n",
    "\n",
    "Here's a template for collecting comprehensive research data using OpenAlex filters and advanced search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-project-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_comprehensive_research_data(topics, years_range=(2020, 2024), min_citations=5):\n",
    "    \"\"\"Collect comprehensive research data for multiple topics with filters\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for topic in topics:\n",
    "        print(f\"Collecting data for: {topic}\")\n",
    "        \n",
    "        # Build filters\n",
    "        filters = f\"publication_year:{years_range[0]}-{years_range[1]},cited_by_count:>{min_citations}\"\n",
    "        \n",
    "        # Collect multiple pages\n",
    "        for page in range(1, 6):  # 5 pages = 125 works max\n",
    "            works = openalex.search_works(\n",
    "                query=topic,\n",
    "                filters=filters,\n",
    "                per_page=25,\n",
    "                page=page\n",
    "            )\n",
    "            \n",
    "            if works and 'results' in works:\n",
    "                for work in works['results']:\n",
    "                    work_data = {\n",
    "                        'search_topic': topic,\n",
    "                        'id': work['id'],\n",
    "                        'title': work['title'],\n",
    "                        'publication_year': work['publication_year'],\n",
    "                        'cited_by_count': work['cited_by_count'],\n",
    "                        'type': work['type'],\n",
    "                        'is_open_access': work['open_access']['is_oa'],\n",
    "                        'oa_type': work['open_access'].get('oa_type', 'closed'),\n",
    "                        'journal': work['primary_location']['source']['display_name'] if work['primary_location'] and work['primary_location'].get('source') else None,\n",
    "                        'publisher': work['primary_location']['source'].get('publisher') if work['primary_location'] and work['primary_location'].get('source') else None,\n",
    "                        'num_authors': len(work['authorships']),\n",
    "                        'first_author': work['authorships'][0]['author']['display_name'] if work['authorships'] else None,\n",
    "                        'first_author_institution': work['authorships'][0]['institutions'][0]['display_name'] if work['authorships'] and work['authorships'][0]['institutions'] else None,\n",
    "                        'top_concept': max(work['concepts'], key=lambda x: x['score'])['display_name'] if work['concepts'] else None,\n",
    "                        'top_concept_score': max(work['concepts'], key=lambda x: x['score'])['score'] if work['concepts'] else None,\n",
    "                        'abstract_length': len(work.get('abstract_inverted_index', {})) if work.get('abstract_inverted_index') else 0,\n",
    "                        'has_doi': bool(work.get('doi')),\n",
    "                        'language': work.get('language', 'unknown')\n",
    "                    }\n",
    "                    all_data.append(work_data)\n",
    "            \n",
    "            time.sleep(0.1)  # Rate limiting\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "# Example usage for final project\n",
    "# research_areas = ['machine learning', 'deep learning', 'natural language processing']\n",
    "# comprehensive_df = collect_comprehensive_research_data(\n",
    "#     research_areas,\n",
    "#     years_range=(2022, 2024),\n",
    "#     min_citations=10\n",
    "# )\n",
    "# \n",
    "# # Save the data\n",
    "# comprehensive_df.to_csv('comprehensive_research_data.csv', index=False)\n",
    "# print(f\"Collected {len(comprehensive_df)} research works\")\n",
    "# print(f\"Unique journals: {comprehensive_df['journal'].nunique()}\")\n",
    "# print(f\"Unique institutions: {comprehensive_df['first_author_institution'].nunique()}\")\n",
    "\n",
    "print(\"Template ready for comprehensive data collection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-points",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* OpenAlex provides free access to millions of scholarly works, authors, institutions, and research topics\n",
    "* No API key required, but including your email is recommended for better performance\n",
    "* Rich metadata includes citations, open access status, author affiliations, and research concepts\n",
    "* Advanced filtering allows you to narrow searches by year, citation count, publication type, and more\n",
    "* Perfect for bibliometric analysis, research trend studies, and academic network analysis\n",
    "* Author and institution data includes h-index, total citations, and research area classifications\n",
    "* Research topics are automatically classified and can reveal interdisciplinary connections\n",
    "* Network analysis can uncover collaboration patterns and research communities\n",
    "* Excellent data source for final projects in digital humanities, science studies, or research analytics\n",
    "  \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}