{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Web APIs: Accessing News Data with NewsAPI\n",
    "\n",
    "* * * \n",
    "\n",
    "### Icons used in this notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive excercise. We'll work through these in the workshop!<br>\n",
    "‚ö†Ô∏è **Warning**: Heads-up about tricky stuff or common mistakes.<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "üé¨ **Demo**: Showing off something more advanced ‚Äì so you know what Python can be used for!<br>\n",
    "\n",
    "### Learning Objectives\n",
    "1. [Setting up NewsAPI](#newsapi)\n",
    "2. [Getting Headlines and Sources](#headlines)\n",
    "3. [Searching for Specific Topics](#search)\n",
    "4. [Analyzing News Data](#analysis)\n",
    "5. [Demo: News Sentiment Over Time](#demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='newsapi'></a>\n",
    "\n",
    "# NewsAPI\n",
    "\n",
    "NewsAPI is a simple and easy-to-use API that returns JSON search results for current and historic news articles published by over 80,000 worldwide sources. It's perfect for tracking news trends, sentiment analysis, and staying updated on specific topics.\n",
    "\n",
    "## Getting Your API Key\n",
    "\n",
    "Before proceeding, you'll need to:\n",
    "1. Visit https://newsapi.org/\n",
    "2. Click \"Get API Key\" and sign up for a free account\n",
    "3. Copy your API key from your account dashboard\n",
    "\n",
    "‚ö†Ô∏è **Warning**: The free tier has limitations (500 requests/day, articles from last 30 days only). For production use, consider upgrading to a paid plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Required Libraries\n",
    "\n",
    "We'll use the `newsapi-python` library for easier interaction with the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install newsapi-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling API Keys\n",
    "\n",
    "Let's securely store and retrieve our NewsAPI key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "def get_api_key(api_name):\n",
    "    config_file_path = os.path.expanduser(\"~/.notebook-api-keys\")\n",
    "    config = configparser.ConfigParser(interpolation=None)\n",
    "    \n",
    "    if os.path.exists(config_file_path):\n",
    "        config.read(config_file_path)\n",
    "    \n",
    "    # Check if API key is present\n",
    "    if config.has_option(\"API_KEYS\", api_name):\n",
    "        update_key = input(f\"An API key for {api_name} already exists. Do you want to update it? (y/n): \").lower()\n",
    "        if update_key == 'n':\n",
    "            return config.get(\"API_KEYS\", api_name)\n",
    "    \n",
    "    # Get new API key\n",
    "    api_key = getpass(f\"Enter your {api_name} API key: \")\n",
    "\n",
    "    # Save the API key\n",
    "    if not config.has_section(\"API_KEYS\"):\n",
    "        config.add_section(\"API_KEYS\")\n",
    "    config.set(\"API_KEYS\", api_name, api_key)\n",
    "    \n",
    "    with open(config_file_path, \"w\") as f:\n",
    "        config.write(f)\n",
    "    \n",
    "    return api_key\n",
    "\n",
    "# Get NewsAPI key\n",
    "newsapi_key = get_api_key(\"NEWSAPI\")\n",
    "print(\"NewsAPI key retrieved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize NewsAPI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "\n",
    "# Initialize the client\n",
    "newsapi = NewsApiClient(api_key=newsapi_key)\n",
    "print(\"NewsAPI client initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='headlines'></a>\n",
    "\n",
    "# Getting Headlines and Sources\n",
    "\n",
    "Let's start by exploring what NewsAPI offers us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Headlines\n",
    "\n",
    "We can get the current top headlines from various countries and categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top headlines from the US\n",
    "top_headlines = newsapi.get_top_headlines(country='us', page_size=10)\n",
    "\n",
    "print(f\"Total results: {top_headlines['totalResults']}\")\n",
    "print(f\"Articles retrieved: {len(top_headlines['articles'])}\")\n",
    "\n",
    "# Look at the first article\n",
    "first_article = top_headlines['articles'][0]\n",
    "print(f\"\\nFirst headline: {first_article['title']}\")\n",
    "print(f\"Source: {first_article['source']['name']}\")\n",
    "print(f\"Published: {first_article['publishedAt']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the structure of an article\n",
    "first_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available News Sources\n",
    "\n",
    "Let's explore what news sources are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all sources\n",
    "sources = newsapi.get_sources()\n",
    "\n",
    "print(f\"Total sources: {len(sources['sources'])}\")\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df_sources = pd.DataFrame(sources['sources'])\n",
    "print(f\"\\nColumns: {df_sources.columns.tolist()}\")\n",
    "df_sources.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sources by category and country\n",
    "print(\"Sources by category:\")\n",
    "print(df_sources['category'].value_counts())\n",
    "\n",
    "print(\"\\nSources by country:\")\n",
    "print(df_sources['country'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headlines by Category\n",
    "\n",
    "üí° **Tip**: NewsAPI supports these categories: `business`, `entertainment`, `general`, `health`, `science`, `sports`, `technology`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get technology headlines\n",
    "tech_headlines = newsapi.get_top_headlines(category='technology', country='us', page_size=15)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_tech = pd.json_normalize(tech_headlines['articles'])\n",
    "print(f\"Retrieved {len(df_tech)} technology articles\")\n",
    "\n",
    "# Show titles and sources\n",
    "for idx, row in df_tech[['title', 'source.name']].head().iterrows():\n",
    "    print(f\"{idx+1}. {row['title']} - {row['source.name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge: Compare Categories\n",
    "\n",
    "- Get headlines from 3 different categories\n",
    "- Compare the number of articles available\n",
    "- Which category has the most diverse sources?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='search'></a>\n",
    "\n",
    "# Searching for Specific Topics\n",
    "\n",
    "The real power of NewsAPI comes from searching for specific topics across all available sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for articles about climate change\n",
    "climate_articles = newsapi.get_everything(\n",
    "    q='climate change',\n",
    "    language='en',\n",
    "    sort_by='publishedAt',\n",
    "    page_size=50\n",
    ")\n",
    "\n",
    "print(f\"Found {climate_articles['totalResults']} articles about climate change\")\n",
    "print(f\"Retrieved {len(climate_articles['articles'])} articles\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_climate = pd.json_normalize(climate_articles['articles'])\n",
    "df_climate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Search with Date Ranges\n",
    "\n",
    "Let's search for articles within a specific time period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate date range (last 7 days)\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=7)\n",
    "\n",
    "# Format dates for API (YYYY-MM-DD)\n",
    "from_date = start_date.strftime('%Y-%m-%d')\n",
    "to_date = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Searching from {from_date} to {to_date}\")\n",
    "\n",
    "# Search for AI articles in the last week\n",
    "ai_articles = newsapi.get_everything(\n",
    "    q='artificial intelligence OR machine learning',\n",
    "    from_param=from_date,\n",
    "    to=to_date,\n",
    "    language='en',\n",
    "    sort_by='popularity',\n",
    "    page_size=30\n",
    ")\n",
    "\n",
    "print(f\"Found {ai_articles['totalResults']} AI articles in the last week\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_ai = pd.json_normalize(ai_articles['articles'])\n",
    "df_ai.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search with Source Filtering\n",
    "\n",
    "We can also limit our search to specific news sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get articles from specific sources\n",
    "sources_list = 'bbc-news,cnn,reuters,the-guardian,the-new-york-times'\n",
    "\n",
    "politics_articles = newsapi.get_everything(\n",
    "    q='election OR politics',\n",
    "    sources=sources_list,\n",
    "    language='en',\n",
    "    sort_by='publishedAt',\n",
    "    page_size=25\n",
    ")\n",
    "\n",
    "print(f\"Found {politics_articles['totalResults']} political articles from major sources\")\n",
    "\n",
    "df_politics = pd.json_normalize(politics_articles['articles'])\n",
    "\n",
    "# Show distribution by source\n",
    "print(\"\\nArticles by source:\")\n",
    "print(df_politics['source.name'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analysis'></a>\n",
    "\n",
    "# Analyzing News Data\n",
    "\n",
    "Now let's perform some analysis on our collected news data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publication Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert publication dates to datetime\n",
    "df_ai['publishedAt'] = pd.to_datetime(df_ai['publishedAt'])\n",
    "df_ai['hour'] = df_ai['publishedAt'].dt.hour\n",
    "df_ai['day'] = df_ai['publishedAt'].dt.day_name()\n",
    "\n",
    "# Analyze publication timing\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "df_ai['hour'].hist(bins=24, alpha=0.7)\n",
    "plt.xlabel('Hour of Day (UTC)')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.title('AI Articles by Hour of Publication')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_counts = df_ai['day'].value_counts().reindex(day_order)\n",
    "day_counts.plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.title('AI Articles by Day of Week')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "df_ai['source.name'].value_counts().head(10).plot(kind='barh', alpha=0.7)\n",
    "plt.xlabel('Number of Articles')\n",
    "plt.title('Top 10 Sources for AI Articles')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Analysis\n",
    "\n",
    "Let's analyze the content of headlines and descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries for text analysis\n",
    "%pip install wordcloud textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "# Combine all titles for word cloud\n",
    "all_titles = ' '.join(df_ai['title'].dropna())\n",
    "\n",
    "# Clean text (remove common words, punctuation)\n",
    "all_titles = re.sub(r'[^\\w\\s]', '', all_titles.lower())\n",
    "\n",
    "# Create word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_titles)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Most Common Words in AI Article Headlines', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment for titles and descriptions\n",
    "def get_sentiment(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    blob = TextBlob(str(text))\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "df_ai['title_sentiment'] = df_ai['title'].apply(get_sentiment)\n",
    "df_ai['description_sentiment'] = df_ai['description'].apply(get_sentiment)\n",
    "\n",
    "# Plot sentiment distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df_ai['title_sentiment'].hist(bins=20, alpha=0.7)\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sentiment Distribution of Headlines')\n",
    "plt.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df_ai['description_sentiment'].hist(bins=20, alpha=0.7, color='orange')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sentiment Distribution of Descriptions')\n",
    "plt.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average title sentiment: {df_ai['title_sentiment'].mean():.3f}\")\n",
    "print(f\"Average description sentiment: {df_ai['description_sentiment'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge: Source Comparison\n",
    "\n",
    "- Compare sentiment across different news sources\n",
    "- Which sources tend to be more positive/negative about AI?\n",
    "- Do you notice any patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demo'></a>\n",
    "\n",
    "# üé¨ Demo: News Sentiment Over Time\n",
    "\n",
    "Let's track how sentiment about a topic changes over time by collecting articles from multiple days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_daily_sentiment(query, days_back=7):\n",
    "    \"\"\"Collect articles for each day and calculate daily sentiment\"\"\"\n",
    "    daily_data = []\n",
    "    \n",
    "    for i in range(days_back):\n",
    "        # Calculate date\n",
    "        date = datetime.now() - timedelta(days=i)\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        try:\n",
    "            # Get articles for this specific day\n",
    "            articles = newsapi.get_everything(\n",
    "                q=query,\n",
    "                from_param=date_str,\n",
    "                to=date_str,\n",
    "                language='en',\n",
    "                sort_by='publishedAt',\n",
    "                page_size=20\n",
    "            )\n",
    "            \n",
    "            if articles['articles']:\n",
    "                # Calculate sentiment for this day\n",
    "                sentiments = []\n",
    "                for article in articles['articles']:\n",
    "                    title_sentiment = get_sentiment(article['title'])\n",
    "                    desc_sentiment = get_sentiment(article.get('description', ''))\n",
    "                    # Average of title and description sentiment\n",
    "                    avg_sentiment = (title_sentiment + desc_sentiment) / 2\n",
    "                    sentiments.append(avg_sentiment)\n",
    "                \n",
    "                daily_data.append({\n",
    "                    'date': date,\n",
    "                    'num_articles': len(articles['articles']),\n",
    "                    'avg_sentiment': np.mean(sentiments),\n",
    "                    'sentiment_std': np.std(sentiments)\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error for {date_str}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(daily_data)\n",
    "\n",
    "# Collect sentiment data for a topic\n",
    "sentiment_data = collect_daily_sentiment('cryptocurrency', days_back=7)\n",
    "print(f\"Collected data for {len(sentiment_data)} days\")\n",
    "sentiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sentiment over time\n",
    "if len(sentiment_data) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(sentiment_data['date'], sentiment_data['avg_sentiment'], marker='o', linewidth=2)\n",
    "    plt.fill_between(sentiment_data['date'], \n",
    "                     sentiment_data['avg_sentiment'] - sentiment_data['sentiment_std'],\n",
    "                     sentiment_data['avg_sentiment'] + sentiment_data['sentiment_std'],\n",
    "                     alpha=0.3)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Average Sentiment')\n",
    "    plt.title('Cryptocurrency News Sentiment Over Time')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(sentiment_data['date'], sentiment_data['num_articles'], alpha=0.7)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.title('Daily Article Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data collected for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Data for Your Final Project\n",
    "\n",
    "Here's a comprehensive template for collecting news data for your final project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_comprehensive_news_data(queries, days_back=30, max_articles_per_query=100):\n",
    "    \"\"\"Collect comprehensive news data for multiple topics over time\"\"\"\n",
    "    all_articles = []\n",
    "    \n",
    "    # Calculate date range\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=days_back)\n",
    "    \n",
    "    from_date = start_date.strftime('%Y-%m-%d')\n",
    "    to_date = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"Collecting articles for: {query}\")\n",
    "        \n",
    "        try:\n",
    "            # Get articles for this query\n",
    "            articles = newsapi.get_everything(\n",
    "                q=query,\n",
    "                from_param=from_date,\n",
    "                to=to_date,\n",
    "                language='en',\n",
    "                sort_by='publishedAt',\n",
    "                page_size=max_articles_per_query\n",
    "            )\n",
    "            \n",
    "            # Process each article\n",
    "            for article in articles['articles']:\n",
    "                article_data = {\n",
    "                    'query': query,\n",
    "                    'title': article['title'],\n",
    "                    'description': article.get('description', ''),\n",
    "                    'content': article.get('content', ''),\n",
    "                    'url': article['url'],\n",
    "                    'source_name': article['source']['name'],\n",
    "                    'source_id': article['source'].get('id', ''),\n",
    "                    'author': article.get('author', ''),\n",
    "                    'published_at': article['publishedAt'],\n",
    "                    'url_to_image': article.get('urlToImage', ''),\n",
    "                    'title_sentiment': get_sentiment(article['title']),\n",
    "                    'description_sentiment': get_sentiment(article.get('description', ''))\n",
    "                }\n",
    "                all_articles.append(article_data)\n",
    "            \n",
    "            print(f\"  Collected {len(articles['articles'])} articles\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error collecting articles for '{query}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_articles)\n",
    "    \n",
    "    if not df.empty:\n",
    "        # Convert date column\n",
    "        df['published_at'] = pd.to_datetime(df['published_at'])\n",
    "        \n",
    "        # Add additional time-based features\n",
    "        df['date'] = df['published_at'].dt.date\n",
    "        df['hour'] = df['published_at'].dt.hour\n",
    "        df['day_of_week'] = df['published_at'].dt.day_name()\n",
    "        \n",
    "        # Remove duplicates based on URL\n",
    "        df = df.drop_duplicates(subset=['url'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage for final project data collection\n",
    "# topics = ['climate change', 'renewable energy', 'carbon emissions', 'environmental policy']\n",
    "# news_df = collect_comprehensive_news_data(topics, days_back=30, max_articles_per_query=50)\n",
    "# news_df.to_csv('environmental_news_data.csv', index=False)\n",
    "# print(f\"Collected {len(news_df)} unique articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* NewsAPI provides access to current and recent news articles from thousands of sources worldwide\n",
    "* You can search by keywords, filter by source, category, language, and date ranges\n",
    "* The API returns structured data including headlines, descriptions, publication dates, and source information\n",
    "* News data is excellent for trend analysis, sentiment tracking, and monitoring public discourse\n",
    "* Free tier limitations mean you should plan your data collection carefully for larger projects\n",
    "* Combining multiple search queries can give you comprehensive coverage of topics\n",
    "* Time-based analysis reveals patterns in news coverage and sentiment shifts\n",
    "  \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}